---
title: "Independent_Project"
author: "Bundi Kirimi"
date: "1/21/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

 library(ggplot2)
 library(tidyverse) 


```


###Exploratory Data Analysis

1.  Defining the Question

<!-- -->

a)  Specifying the Data Analytic Question

Identifying the individuals likely to click on Ads

b)  Defining the Metric for Success Identifying iNdividulas who will likely click on Ads created

c)  Understanding the context

d)Recording the Experimental Design

e)  Data Relevance

<!-- -->

2.  Reading the Data

###Loading Our Datatset

```{R}

library("data.table")


advertising <- fread("http://bit.ly/IPAdvertisingData")

head(advertising)

```

3.  Checking the Data

```{R}

ncol(advertising)
```

```{r}



summary(advertising)
```

```{R}
###Finding the Statistical mean of numeric Columns


table(advertising$`Clicked on Ad`)


table(advertising$`Male`)
```

Equal Number of Clicks on Ads and Non clicks

There are more females to males in the sites,where 1 is Male,0 is Female

```{R}
###Finding the Minimum and Maximum Values on Numerical Columns

##Finding the  minimum values 

#Age column
advertising.Age.min   <- min  (advertising$Age)
advertising.Age.min



###$#$Finding the Max values in Our numerical Columns


#Age
advertising.Age.max   <- max  (advertising$Age)
advertising.Age.max


```

#The youngest Age is 19 and the Oldest age in the website cleint is 61

a)Validation

##Cleaning Our Dataset

```{R}

# Identifying Missing and Null Values
# ---
# 

View(advertising)
```

```{R}

any(is.na(advertising))



colSums(is.na(advertising))

```

There are no Null Values

###Checking for outliers

```{R}
boxplot(advertising$Age)


```

```{R}


boxplot(advertising$`Daily Internet Usage`)
```

```{R}

boxplot(advertising$`Area Income`)
```

There are a few outliers in our data on The Income column that doesnt affect our analysis

###Checking for duplicates

```{R}
#Checkcing for Duplicates and unique values


any(duplicated(advertising))

##Rows

duplicated_rows <- advertising[duplicated(advertising),]




duplicated_rows <- advertising[duplicated(advertising),]
duplicated_rows

```

```{R}

#Seeing Unique values



unique_items <- advertising[!duplicated(advertising), ]

unique_items

```

There are no Duplicates


Changing the timestamp datatype from factor to date_time

```{r}
#changing the timestamp datatype from factor to date_time
advertising$Timestamp <- as.Date(advertising$Timestamp, format = "%Y-%m-%s-%h-%m-
%s")
```

```{r}
#checking the new datatype for the Timestamp column
sapply(advertising, class)
```








###Univariate analysis
Daily.Time.Spent.on.Site
```{r}
#This column represents the amount of time that a user spends on the website
# measures of central tendency
summary(advertising$`Daily Time Spent on Site`)

```
```{r}
#distribution
hist(advertising$`Daily Time Spent on Site`, col=c("darkorange"))
```
The users spend an average 65.002 minutes on the website.
The modal time is “62.26” “75.55” “77.05” “78.76” “84.53”
The median time is 68.215.
The distribution above is left-skewed.
The highest frequency is 80 units of time(minutes).

Age
```{r}
# Age of the user
#This column represents the Age of the user
# measures of central tendency

summary(advertising$Age)
```
```{r}
#distribution
hist(advertising$Age, col = c("pink"))
```
The age distribution is right skewed
The respondents on the website are mostly 25-40 years old.
The mean age is 36.
The median age is 35


AreaIncome
```{r}
# Age of the user
#This column represents the Age of the user
# measures of central tendency

summary(advertising$`Area Income`)
```
```{r}
#distribution
hist(advertising$`Area Income`, col = c("pink"))
```
The income distribution is left skewed
The respondents on the website mostly earn between 55,000 to 70,000.
The mean income is 55,000.
The median income is 57,012.

DailyInternet Usage
```{r}
# Age of the user
#This column represents the Age of the user
# measures of central tendency

summary(advertising$`Daily Internet Usage`)
```
```{r}
#distribution
hist(advertising$`Daily Internet Usage`, col = c("pink"))
```
Ad Topic Line
```{r}
#
Ad_topic_line <- advertising$Ad.Topic.Line
#all the values are unique in this column thus we would drop it whenmodelling since it
#does not provide any additional meaningful information
levels(unique(Ad_topic_line))

factor(unique(Ad_topic_line))

```


Male
```{r}

#gender of the user
#1 indicates that the user is male while indicates that they are female
# measures of central tendency
#levels(advert_df$Male) #this code does not work
#obtaining the unique levels in the gender(Male column)
unique(factor(advertising$Male))

```

```{r}
Male_x <- table(advertising$Male)
#distribution
barplot(Male_x, main="Gender Distribution",col=c("darkgreen"),xlab="Gender")
```
Clicked.on.Ad
```{r}
#zero indicates that a user did not click on an add while 1 indicates that auser clicked on an add
# measures of central tendency

table(advertising$`Clicked on Ad`)
```

```{r}
Male_x <- table(advertising$`Clicked on Ad`)
#distribution
barplot(Male_x, main="Clicks Distribution",col=c("darkgreen"),xlab="Click.on.Ad")
```


BVivariate Analysis
```{R}




head(advertising)

df2 <- subset(advertising, select = c(1,2,3,4,7,10))

df2
```



Correlation

`````{R}


#The default method is Pearson, but we can also compute Spearman or Kendallcoefficients.
mydata = cor(df2, method = c("spearman"))
mydata1= cor(df2, method = c("kendall"))
mydata2= cor(df2, method = c("pearson"))

mydata #spearman
`````

```{r}
mydata1 #kendall
```


```{R}
mydata2 #pearson
```

Using the 3 correlation coefficients to get the correlation between the features, we can see
that the correlation is very low and negative in most cases.
This means that most of the variables are NOT dependent of each other


#Generating P values
```{r}
#install_version("latticeExtra")
#install.packages("Hmisc", dependencies = T)
library("Hmisc")

```

```{r}
mydata.rcorr = rcorr(as.matrix(mydata)) #feed the data as a matrix
mydata.rcorr


```

```{r}
#mydata.coeff = mydata.rcorr$r
#mydata.p = mydata.rcorr$P
library(corrplot)
## corrplot 0.84 loaded
corrplot(mydata)
```
Positive
correlations are displayed in a blue scale while negative correlations are displayed
in a red scale
There is very minimal positive correlation between the variables in the data
scatterplots of a few pairs of variables

```{R}





plot( advertising$`Age`,advertising$`Daily Time Spent on Site`, xlab = "Age", ylab = "Daily time Spent on Sites ", col = "red")

```
#positive non-linear correlation
People between the age 40 and below spend much time on the sites While people aged 50 and above spend less time on sites

```{R}

#create scatterplot of Daily Internet Usage vs. Time spent on site 
plot(advertising$`Daily Internet Usage`, advertising$`Daily Time Spent on Site`, pch=16, col='steelblue',
     main=' Daily Internet Usage vs. Daily Time spent on site',
     xlab='Daily Internet Usage', ylab='Daily Time spent on site')

```

People with high daily internet usage spend higher time on sites
```{r}
#create scatterplot of Daily Internet Usage vs. Time spent on site 
plot(advertising$`Daily Internet Usage`, advertising$`Age`, pch=16, col='steelblue',
     main=' Daily Internet Usage vs.Age',
     xlab='Daily Internet Usage', ylab='Age')

```
#the plot shows that there is positive non-linear correlation

```{R}






plot(advertising$`Area Income` ,advertising$`Age`, xlab= "Clicked on Ad", ylab = "Age", col = "red")




```

People with high Area income are aged between 30 and Above

```{R}

plot( advertising$`Daily Time Spent on Site`,advertising$`Area Income`, xlab = "Daily time Spent on Sites ", ylab = "Area Income", col = "red")

```
#positive non-linear correlation
People with High area income spend more time on sites

```{R}


plot( advertising$`Daily Internet Usage`,advertising$Age, xlab = "Daily time Spent on Sites ", ylab = "Age", col = "red")

```

People aged 30 and below have high internet usage
Seperating the data Clicked and Gender columns

````{r}

#populating the column with false values from the male column
df2$Female <- df2$Male == 0

dim(df2)

#converting the column to nu,eric
dim(df2 <- apply(df2, 2, as.numeric))


```

Gender VS Clicked on Add
```{r}

library(tidyverse)

#Male respondents who clicked on an add
dim(advertising%>% filter(Male == 1 , `Clicked on Ad` == 1))

#Male respondents did not click on an add
dim(advertising%>% filter(Male == 1, `Clicked on Ad` == 0))

#Female respondents who clicked on an add
dim(advertising%>% filter(Male == 0 , `Clicked on Ad` == 1))
#Female respondents who clicked did not on an add
dim(advertising%>% filter(Male == 0, `Clicked on Ad` == 0))
```
```{r}
Clicked_vs_gender <- c( 231 , 250 , 269 , 250 )

# barchart with added parameters
barplot(Clicked_vs_gender, main = " Clicked_vs_gender " , xlab = " Label ",
ylab = " Count ",
names.arg = c("Male&Clicked Male&No-click Female&Clicked Female&No-Click"),
col = "Orange",
horiz = FALSE)
```
Multivariate Analysis

```{R}
# A glimpse of the data
library(dplyr)
glimpse(df2)

```



```{R}
# One hot encoding of the factor variables.
library(caret)
## Loading required package: lattice
dmy <- dummyVars(" ~ .", data = df2)
dummy_df <- data.frame(predict(dmy, newdata = df2))
#print(dummy_df)
glimpse(dummy_df)
```
```{r}
sapply(dummy_df, class)
```
```{r}
dummy_df2 <-dummy_df[c(1,2,3,4,5,6)]

dim(dummy_df2)
#6 columns in dummy_df2
```

```{r}
dummy_df.class<- df2[,6]


```


Scaling


```{r}
dummy_df2_scaled <- scale(dummy_df2)
summary(dummy_df2_scaled)
```
```{r}
dummy_df2_norm <- as.data.frame(apply(dummy_df2, 2, function(x) (x -
min(x))/(max(x)-min(x))))
summary(dummy_df2_norm)
```

Plotting Percentage of Varinace
```{r}
library(factoextra)
## Welcome! Want to learn more? See two factoextra-related books at

distance <- get_dist(dummy_df2_norm)
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high ="#FC4E07"))
```

Finding the Optimal number of clusters
```{r}
# Elbow method
# Searching for the optimal number of clusters
# # Elbow method
# Searching for the optimal number of clusters
# # Elbow method
library(factoextra)

fviz_nbclust(dummy_df2_norm, kmeans, method = "wss") +
 geom_vline(xintercept = 4, linetype = 2)+
 labs(subtitle = "Elbow method")
```



Implement the Solution
K-MEANS CLUSTERING
Using 4 clusters [Elbow Method]

```{r}
outputk <- kmeans(dummy_df2_norm, 4)
```

```{r}
outputk$size

outputk$centers

```

Visualising the clusters of the whole dataset

```{r}
options(repr.plot.width = 11, repr.plot.height = 6)
fviz_cluster(outputk, dummy_df2_norm)
```


while using four points, we can see that the data is divided into two distinct clusters first
then two more clusters from the two.


Visualizing variable datatypes on a scatter plot

```{r}
# Plotting two variables to see how their data points
# have been distributed in the cluster
# Product Related, vs Product Related Duration
plot(dummy_df2_norm[, 5:6], col = outputk$cluster)
```
HIERACHICAL CLUSTERING
```{r}

d <- dist(dummy_df2_norm, method = "euclidean")
# We then apply hierarchical clustering using the Ward's method
res.hc <- hclust(d, method = "ward.D2")
# Lastly we plot the obtained dendrogram
#--
plot(res.hc, cex = 0.6, hang = -1)

```
Challenge the Solution
1. PCA
```{r}
# Reducing the dimensionality of the dataset
library(ggbiplot)

```

```{r}
pca_residual = prcomp(dummy_df2_norm, scale = T, center = T)
# Visualising the pca results
options(repr.plot.width = 6, repr.plot.height = 6)
ggbiplot(pca_residual) +
 labs(title = 'Explained variance plot')
```

```{r}
# Applying PCA
# We pass df_norm to the prcomp().
# We also set two arguments, center and scale,
# to be TRUE then preview our object with summary
dummy_PCA <- prcomp(dummy_df2_norm,
 center = TRUE,
 scale = FALSE)
summary(dummy_PCA)

```

The first two pricipal components explain about 85% of the variance in the data.
The first four pricipal components explain about 94% of the variance in the data.
```{r}
library("FactoMineR")
library("factoextra")

fviz_pca_var(pca_residual, col.var = "cos2",
 gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
 repel = TRUE # Avoid text overlapping)
)
```

Variables that are closed to the center of the plot are less important for the first
components.
The most important pair is the Gender MALE VS FEMALE
The Second most important pair is the Daily time spent on the site AND Daily Internet
Usage
Lastly, the third most important pair is the Age And Income

The Principal Components and how well they explain the variance
```{r}
var <- get_pca_var(pca_residual)
head(var$contrib, 4)

```
For the First principal component, the Daily internet usage and amount spent ont the site
explain more that 50% of the variance.
In the second principal component, the Daily internet usage and amount spent ont the site
explain more that 9% of the variance
In the Third Principal component, Age and income explain almost 100% of the valriance
From this therefore, We can order the components based on how well they explain the
variance as:
a) Daily Internet Usage
b) Daily time spent on the site
c) Income
d) Age

he PCA explains the following properties about the data


```{r}

library("FactoMineR")
library("factoextra")
fviz_eig(pca_residual, barfill = 'cyan',linecolor = 'red' )
```
From the plot above, the elbow forms after the 2nd and 4th dimensions. This indicates that
the analysis should yield 2 or 4 major factors.

Propwerties of the data based on PCA

```{r}
var <- get_pca_var(pca_residual)
var
```
2. K-MEANS CLUSTERING
Using a different number of clusters 2 clusters using the silhouette method
Using 2 clusters [Silhouette Method]
```{r}
outputs <- kmeans(dummy_df2_norm, 2)
```

```{r}
outputs$size

```
Visualising the clusters of the whole dataset

```{r}
options(repr.plot.width = 11, repr.plot.height = 6)
fviz_cluster(outputs, dummy_df2_norm)
```




```{r}
library("car")

scatterplotMatrix(dummy_df2_norm[1:4])
```


Summary
Compasiron Between K-MEANS and HIERACHICAL clustering From the Analysis, we can
identify that:

1. K-means Cluster Analysis performs much better in identyfing patterns as compared to
Hierrachical clustering.

2. Since the dataset is large, visualizing hierrachical clusters is abit cumbersome as
compared to K-means clustering.

3. K-means clustering yields better reults using the optimal number of clusters which can
be determined by Elbow and Silhouette Methods

4. Clicking on an add is dependent on the gender of the respondent

5. We can conclude that,The order of the factors that affect if a respondent clicks on an ad
is:
 Gender
 Daily Internet Usage
 Daily time spent on the site
 Income
 Age












